{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb061f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from librosa.util import normalize\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6c1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    log_interval = 10\n",
    "    loss_func = CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.repeat(1, 3, 1, 1)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f00446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_func = CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.repeat(1, 3, 1, 1)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_func(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f21a170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_out(model, device, out_loader):\n",
    "    model.eval()  \n",
    "    output_vals = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(out_loader):\n",
    "            data = data.repeat(1, 3, 1, 1)\n",
    "            data = data.to(device)  \n",
    "            output = model(data)  \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            output_vals += list(pred.cpu().numpy()[:, 0])\n",
    "    return output_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14064ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSounds(Dataset):\n",
    "\n",
    "    def __init__(self, root_path, names, sounds, labels,  transform=None):\n",
    "        self.labels = labels\n",
    "        self.sounds = sounds\n",
    "        self.names = names\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sr = librosa.load(self.root_path + '/' + self.sounds[index] + '/' + self.names[index])\n",
    "        label = self.labels[index]\n",
    "        n_fft = 1024\n",
    "        step = n_fft // 4\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform,\n",
    "                                                         sr=sr,\n",
    "                                                         n_fft=n_fft,\n",
    "                                                         hop_length=step)\n",
    "        mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrogram = np.round(mel_spectrogram)\n",
    "        mel_spectrogram = Image.fromarray(mel_spectrogram)\n",
    "        if self.transform is not None:\n",
    "             mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f97f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTest(Dataset):\n",
    "\n",
    "    def __init__(self, root_path, csv,  transform=None):\n",
    "        self.root_path = root_path\n",
    "        self.data = pd.read_csv(csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sr = librosa.load(self.root_path + '/' + self.data.loc[index, 'id'])\n",
    "        label = self.data.loc[index, 'answer']\n",
    "        n_fft = 1024\n",
    "        step = n_fft // 4\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform,\n",
    "                                                         sr=sr,\n",
    "                                                         n_fft=n_fft,\n",
    "                                                         hop_length=step)\n",
    "        mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrogram = np.round(mel_spectrogram)\n",
    "        mel_spectrogram = Image.fromarray(mel_spectrogram)\n",
    "        if self.transform is not None:\n",
    "             mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "038615b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu Epochs: 4 Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "dict_of_sounds_indexes = {\n",
    "    0: 'stop',\n",
    "    1: 'one',\n",
    "    2: 'two',\n",
    "    3: 'three',\n",
    "    4: 'four',\n",
    "    5: 'five',\n",
    "    6: 'six',\n",
    "    7: 'cat',\n",
    "    8: 'dog',\n",
    "    9: 'house'\n",
    "}\n",
    "\n",
    "dict_of_sounds = {\n",
    "    'stop': 0,\n",
    "    'one': 1,\n",
    "    'two': 2,\n",
    "    'three': 3,\n",
    "    'four': 4,\n",
    "    'five': 5,\n",
    "    'six': 6,\n",
    "    'cat': 7,\n",
    "    'dog': 8,\n",
    "    'house': 9\n",
    "}\n",
    "\n",
    "sounds_path = sorted(glob.glob('train/*/*.wav'))\n",
    "\n",
    "sounds = []\n",
    "names =[]\n",
    "labels = []\n",
    "\n",
    "for path in sounds_path:\n",
    "    sound = os.path.basename(os.path.dirname(path))\n",
    "    name = os.path.basename(path)\n",
    "    sounds.append(sound)\n",
    "    names.append(name)\n",
    "\n",
    "for sound in sounds:\n",
    "    labels.append(sound)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = dict_of_sounds[labels[i]]\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 1.0\n",
    "reduce_lr_gamma = 0.7\n",
    "epochs = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: {} Epochs: {} Batch size: {}'.format(device, epochs, batch_size))\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {'batch_size': batch_size}\n",
    "if torch.cuda.is_available():\n",
    "    kwargs.update({'num_workers': 1, 'pin_memory': True})\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = DatasetSounds(root_path='train',\n",
    "                       names=names,\n",
    "                       sounds=sounds,\n",
    "                       labels=labels,\n",
    "                       transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "out_dataset = DatasetTest(root_path='test',\n",
    "                                   csv='sample_submission.csv',\n",
    "                                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, **kwargs)\n",
    "out_loader = torch.utils.data.DataLoader(out_dataset, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee19651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu Epochs: 4 Batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupyter\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\jupyter\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/11832 (0%)]\tLoss: 2.369676\n",
      "Train Epoch: 1 [1280/11832 (11%)]\tLoss: 2.608781\n",
      "Train Epoch: 1 [2560/11832 (22%)]\tLoss: 0.805700\n",
      "Train Epoch: 1 [3840/11832 (32%)]\tLoss: 0.161071\n",
      "Train Epoch: 1 [5120/11832 (43%)]\tLoss: 0.274030\n",
      "Train Epoch: 1 [6400/11832 (54%)]\tLoss: 0.497340\n",
      "Train Epoch: 1 [7680/11832 (65%)]\tLoss: 0.179783\n",
      "Train Epoch: 1 [8960/11832 (75%)]\tLoss: 0.350849\n",
      "Train Epoch: 1 [10240/11832 (86%)]\tLoss: 0.065113\n",
      "Train Epoch: 1 [11520/11832 (97%)]\tLoss: 0.236561\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 1210/1315 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/11832 (0%)]\tLoss: 0.123031\n",
      "Train Epoch: 2 [1280/11832 (11%)]\tLoss: 0.070359\n",
      "Train Epoch: 2 [2560/11832 (22%)]\tLoss: 0.089164\n",
      "Train Epoch: 2 [3840/11832 (32%)]\tLoss: 0.062905\n",
      "Train Epoch: 2 [5120/11832 (43%)]\tLoss: 0.059302\n",
      "Train Epoch: 2 [6400/11832 (54%)]\tLoss: 0.183414\n",
      "Train Epoch: 2 [7680/11832 (65%)]\tLoss: 0.200512\n",
      "Train Epoch: 2 [8960/11832 (75%)]\tLoss: 0.094612\n",
      "Train Epoch: 2 [10240/11832 (86%)]\tLoss: 0.249644\n",
      "Train Epoch: 2 [11520/11832 (97%)]\tLoss: 0.077974\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 1193/1315 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/11832 (0%)]\tLoss: 0.189467\n",
      "Train Epoch: 3 [1280/11832 (11%)]\tLoss: 0.057804\n",
      "Train Epoch: 3 [2560/11832 (22%)]\tLoss: 0.059914\n",
      "Train Epoch: 3 [3840/11832 (32%)]\tLoss: 0.127124\n",
      "Train Epoch: 3 [5120/11832 (43%)]\tLoss: 0.018233\n",
      "Train Epoch: 3 [6400/11832 (54%)]\tLoss: 0.123649\n",
      "Train Epoch: 3 [7680/11832 (65%)]\tLoss: 0.018423\n",
      "Train Epoch: 3 [8960/11832 (75%)]\tLoss: 0.080599\n",
      "Train Epoch: 3 [10240/11832 (86%)]\tLoss: 0.049056\n",
      "Train Epoch: 3 [11520/11832 (97%)]\tLoss: 0.023455\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 1283/1315 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/11832 (0%)]\tLoss: 0.055465\n",
      "Train Epoch: 4 [1280/11832 (11%)]\tLoss: 0.048484\n",
      "Train Epoch: 4 [2560/11832 (22%)]\tLoss: 0.071180\n",
      "Train Epoch: 4 [3840/11832 (32%)]\tLoss: 0.001570\n",
      "Train Epoch: 4 [5120/11832 (43%)]\tLoss: 0.020765\n",
      "Train Epoch: 4 [6400/11832 (54%)]\tLoss: 0.018853\n",
      "Train Epoch: 4 [7680/11832 (65%)]\tLoss: 0.026788\n",
      "Train Epoch: 4 [8960/11832 (75%)]\tLoss: 0.094054\n",
      "Train Epoch: 4 [10240/11832 (86%)]\tLoss: 0.095476\n",
      "Train Epoch: 4 [11520/11832 (97%)]\tLoss: 0.049350\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 1297/1315 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=10)\n",
    "model.to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=reduce_lr_gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test_model(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b55b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1835e0131646489789e4a77d853ef6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = form_out(model, device, out_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "619e0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output)):\n",
    "    output[i] = dict_of_sounds_indexes[output[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f0d4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')\n",
    "result = pd.DataFrame(sample['id'], columns=['id'])\n",
    "result['answer'] = output\n",
    "result.to_csv('result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45a8e518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9065</th>\n",
       "      <td>9065.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>9066.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>9067.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9068</th>\n",
       "      <td>9068.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9069</th>\n",
       "      <td>9069.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9070 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id answer\n",
       "0        0.wav    cat\n",
       "1        1.wav    cat\n",
       "2        2.wav    cat\n",
       "3        3.wav    cat\n",
       "4        4.wav    cat\n",
       "...        ...    ...\n",
       "9065  9065.wav    cat\n",
       "9066  9066.wav    cat\n",
       "9067  9067.wav    cat\n",
       "9068  9068.wav    cat\n",
       "9069  9069.wav    cat\n",
       "\n",
       "[9070 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bc4677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.wav</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.wav</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.wav</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.wav</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9065</th>\n",
       "      <td>9065.wav</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>9066.wav</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>9067.wav</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9068</th>\n",
       "      <td>9068.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9069</th>\n",
       "      <td>9069.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9070 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id answer\n",
       "0        0.wav   four\n",
       "1        1.wav    one\n",
       "2        2.wav    two\n",
       "3        3.wav    dog\n",
       "4        4.wav    six\n",
       "...        ...    ...\n",
       "9065  9065.wav    one\n",
       "9066  9066.wav    dog\n",
       "9067  9067.wav  house\n",
       "9068  9068.wav    two\n",
       "9069  9069.wav  three\n",
       "\n",
       "[9070 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a77023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
